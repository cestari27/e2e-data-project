{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a4ded4-8725-46c1-a0c2-c4ed531e8d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This notebook is meant to train a classification model from the Iris dataset and save it to the UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c7beae74-8e38-40e5-a07a-f8b53bd21541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.2.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-tracing==3.2.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (3.7.2)\n",
      "Requirement already satisfied: numpy<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.23.5)\n",
      "Requirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.5.3)\n",
      "Requirement already satisfied: pyarrow<22,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (14.0.1)\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.3.0)\n",
      "Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.11.1)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading sqlalchemy-2.0.42-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (3.0.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.40.0)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (6.0.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (23.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (5.29.3)\n",
      "Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading pydantic-2.12.0a1-py3-none-any.whl.metadata (76 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (2.31.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (4.10.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.16)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jinja2>=3.1.2 (from Flask<4->mlflow)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe>=2.1.1 (from Flask<4->mlflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (2.35.0)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.2.0->mlflow) (3.11.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.37.2 (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading pydantic_core-2.37.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (2023.7.22)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (4.9)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (0.4.8)\n",
      "Downloading mlflow-3.2.0-py3-none-any.whl (25.8 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/25.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m18.1/25.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m25.2/25.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.2.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.2.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading sqlalchemy-2.0.42-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading pydantic-2.12.0a1-py3-none-any.whl (453 kB)\n",
      "Downloading pydantic_core-2.37.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, markupsafe, itsdangerous, h11, gunicorn, greenlet, graphql-core, click, blinker, annotated-types, werkzeug, uvicorn, typing-inspection, sqlalchemy, pydantic-core, opentelemetry-api, Mako, jinja2, graphql-relay, docker, anyio, starlette, pydantic, opentelemetry-semantic-conventions, graphene, Flask, alembic, opentelemetry-sdk, fastapi, mlflow-tracing, mlflow-skinny, mlflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083\n",
      "    Can't uninstall 'click'. No files were found to uninstall.\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.4\n",
      "    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083\n",
      "    Can't uninstall 'blinker'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.6\n",
      "    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083\n",
      "    Can't uninstall 'pydantic'. No files were found to uninstall.\n",
      "  Attempting uninstall: mlflow-skinny\n",
      "    Found existing installation: mlflow-skinny 2.11.4\n",
      "    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083\n",
      "    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n",
      "Successfully installed Flask-3.1.1 Mako-1.3.10 alembic-1.16.4 annotated-types-0.7.0 anyio-4.10.0 blinker-1.9.0 click-8.2.1 docker-7.1.0 fastapi-0.116.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 gunicorn-23.0.0 h11-0.16.0 itsdangerous-2.2.0 jinja2-3.1.6 markupsafe-3.0.2 mlflow-3.2.0 mlflow-skinny-3.2.0 mlflow-tracing-3.2.0 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 pydantic-2.12.0a1 pydantic-core-2.37.2 sniffio-1.3.1 sqlalchemy-2.0.42 starlette-0.47.2 typing-extensions-4.14.1 typing-inspection-0.4.1 uvicorn-0.35.0 werkzeug-3.1.3\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow --upgrade --pre\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b781b85-5fb9-4af5-8642-f5f3fb3993a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94d72ec-d6d4-4fd3-9792-cc37870736da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"pedroz_e2edata_dev\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c8ec82-ceec-4e6f-bd77-bc1ba02016d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'iris_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c01992c-8f50-4c1a-872d-8840ac2e2d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table_name = f'{catalog_name}.default.iris_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ea3d22-39fb-48cb-9023-6f5442449f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/{model_name}_{catalog_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9183695-cffb-40d6-9377-a10a5285b1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/3571252969788486', creation_time=1754402701666, experiment_id='3571252969788486', last_update_time=1754412791706, lifecycle_stage='active', name='/Users/pedro.zanlorensi@databricks.com/iris_model_pedroz_e2edata_dev', tags={'mlflow.databricks.filesystem.experiment_permissions_check': 'test',\n",
       " 'mlflow.experiment.sourceName': '/Users/pedro.zanlorensi@databricks.com/iris_model_pedroz_e2edata_dev',\n",
       " 'mlflow.experimentKind': 'custom_model_development',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'pedro.zanlorensi@databricks.com',\n",
       " 'mlflow.ownerId': '4529363229261380'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an MLFlow experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed03c85-3569-48d7-823d-4e73110e36ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/08 16:57:39 WARNING mlflow.utils.autologging_utils: MLflow sklearn autologging is known to be compatible with 1.3.1 <= scikit-learn <= 1.7.1, but the installed version is 1.3.0. If you encounter errors during autologging, try upgrading / downgrading scikit-learn to a compatible version, or try upgrading MLflow.\n",
      "2025/08/08 16:57:40 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/08/08 16:57:40 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: MLflow Spark dataset autologging is not supported on Databricks shared clusters or Databricks serverless clusters.\n",
      "2025/08/08 16:57:40 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark.ml: [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `sparkContext` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session. Visit https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession for creating regular Spark Session in detail.\n",
      "🔗 View Logged Model at: https://adb-4181970831265458.18.azuredatabricks.net/ml/experiments/3571252969788486/models/m-9f7f58d950764a07b65799c8f40b2ebc?o=4181970831265458\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "🔗 View Logged Model at: https://adb-4181970831265458.18.azuredatabricks.net/ml/experiments/3571252969788486/models/m-f613615d6f3a424981824996f896a829?o=4181970831265458\n",
      "2025/08/08 16:58:02 WARNING mlflow.data.spark_delta_utils: Failed to obtain version information for Delta table with name 'pedroz_e2edata_dev.default.iris_data'. Version information may not be included in the dataset source for MLflow Tracking. Exception: [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jvm` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session. Visit https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession for creating regular Spark Session in detail.\n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-de9c4b31-9436-4be9-a101-60d217f02083/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/08/08 16:58:03 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: [NOT_IMPLEMENTED] rdd is not implemented.\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "# Start a training run\n",
    "with mlflow.start_run() as run:\n",
    "    # Load data from Unity Catalog table\n",
    "    df_iris = spark.table(feature_table_name).toPandas()\n",
    "    features = ['sepal_length_cm', 'sepal_width_cm', 'petal_length_cm', 'petal_width_cm']\n",
    "    target = 'species'\n",
    "\n",
    "    X = df_iris[features]\n",
    "    y = df_iris[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "    # Train the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"test_recall\", recall)\n",
    "    mlflow.log_metric(\"test_precision\", precision)\n",
    "    mlflow.log_metric(\"test_f1\", f1)\n",
    "\n",
    "    # Infer model signature\n",
    "    signature = infer_signature(X_train, y_train)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        name=\"model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train.head()\n",
    "    )\n",
    "\n",
    "    # Log input dataset for lineage\n",
    "    data_source = mlflow.data.load_delta(table_name=feature_table_name)\n",
    "    mlflow.log_input(data_source, context=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e85f8e0-b6a0-49da-863c-f090b655a311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'pedroz_e2edata_dev.default.iris_model' already exists. Creating a new version of this model...\n",
      "2025/08/08 16:58:04 WARNING mlflow.tracking._model_registry.fluent: Run with id d6f62b4598754f0392c697d2f8e89e47 has no artifacts at artifact path 'model', registering model based on models:/m-f613615d6f3a424981824996f896a829 instead\n",
      "🔗 Created version '20' of model 'pedroz_e2edata_dev.default.iris_model': https://adb-4181970831265458.18.azuredatabricks.net/explore/data/models/pedroz_e2edata_dev/default/iris_model/version/20?o=4181970831265458\n"
     ]
    }
   ],
   "source": [
    "# Out of all runs in the experiment, only register the run with the best selected metric\n",
    "# Important note: this logic is optional and totally depends on your processes, so feel free to customize it!\n",
    "# If you want, you can simply register the latest run instead, for example. \n",
    "\n",
    "selected_metric = 'test_accuracy'\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "runs = client.search_runs(experiment_ids=[experiment.experiment_id], order_by=[f\"metrics.{selected_metric} DESC\"], max_results=1)\n",
    "best_run_id = runs[0].info.run_id\n",
    "\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "registered_model = mlflow.register_model(model_uri, f\"{catalog_name}.default.{model_name}\")\n",
    "client.set_registered_model_alias(name=registered_model.name, alias=\"challenger\", version=registered_model.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5753565242613738,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "model-training",
   "widgets": {
    "catalog_name": {
     "currentValue": "pedroz_e2edata_dev",
     "nuid": "c3deff61-8418-4ca2-ae02-acb938772fc8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pedroz_e2edata_dev",
      "label": null,
      "name": "catalog_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "pedroz_e2edata_dev",
      "label": null,
      "name": "catalog_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
